{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f938540",
   "metadata": {},
   "source": [
    "# An end-to-end Vertex Training Pipeline Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7de01",
   "metadata": {},
   "source": [
    "Finally, check that you have correctly installed the packages. The KFP SDK version should be >=1.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9222df33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.18\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "164ff1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "import kfp\n",
    "import pprint\n",
    "import yaml\n",
    "from jinja2 import Template\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.compiler import compiler\n",
    "from kfp.v2.dsl import Dataset, Input, Metrics, Model, Output, component\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from google.cloud import aiplatform, firestore\n",
    "from datetime import datetime\n",
    "from google_cloud_pipeline_components.experimental import hyperparameter_tuning_job\n",
    "from google_cloud_pipeline_components.v1.hyperparameter_tuning_job import HyperparameterTuningJobRunOp\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from kfp.v2.components import importer_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe612cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id='hyu-ml-sandbox'\n",
    "project_number='439762652'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27224ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "af_registry_location='us-central1'\n",
    "af_registry_name='mlops-vertex-kit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe4d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_dir='../components/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82030efb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _load_custom_component(project_id: str,\n",
    "                           af_registry_location: str,\n",
    "                           af_registry_name: str,\n",
    "                           components_dir: str,\n",
    "                           component_name: str):\n",
    "  component_path = os.path.join(components_dir,\n",
    "                                component_name,\n",
    "                                'component.yaml.jinja')\n",
    "  with open(component_path, 'r') as f:\n",
    "    component_text = Template(f.read()).render(\n",
    "      project_id=project_id,\n",
    "      af_registry_location=af_registry_location,\n",
    "      af_registry_name=af_registry_name)\n",
    "\n",
    "  return kfp.components.load_component_from_text(component_text)\n",
    "\n",
    "load_custom_component = partial(_load_custom_component,\n",
    "                                project_id=project_id,\n",
    "                                af_registry_location=af_registry_location,\n",
    "                                af_registry_name=af_registry_name,\n",
    "                                components_dir=components_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "495502ee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_op = load_custom_component(component_name='data_preprocess')\n",
    "train_op = load_custom_component(component_name='train_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0026cb75",
   "metadata": {},
   "source": [
    "Then define the pipeline using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d36a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_region='us-central1'\n",
    "pipeline_root='gs://vertex_pipeline_demo_root_hyu/pipeline_root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "954a7af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_region='us-central1'\n",
    "input_dataset_uri='bq://hyu-ml-sandbox.vertex_pipeline_demo.banknote_authentication'\n",
    "gcs_data_output_folder='gs://vertex_pipeline_demo_root_hyu/datasets/training'\n",
    "training_data_schema='VWT:float;SWT:float;KWT:float;Entropy:float;Class:int'\n",
    "\n",
    "data_pipeline_root='gs://vertex_pipeline_demo_root_hyu/compute_root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6812d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_container_image_uri=f'{af_registry_location}-docker.pkg.dev/{project_id}/{af_registry_name}/training:latest'\n",
    "serving_container_image_uri=f'{af_registry_location}-docker.pkg.dev/{project_id}/{af_registry_name}/serving:latest'\n",
    "hpo_container_image_uri=f'{af_registry_location}-docker.pkg.dev/{project_id}/{af_registry_name}/hpo:latest'\n",
    "custom_job_service_account=f'{project_number}-compute@developer.gserviceaccount.com'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4574529c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('us-central1-docker.pkg.dev/hyu-ml-sandbox/mlops-vertex-kit/training:latest',\n",
       " 'us-central1-docker.pkg.dev/hyu-ml-sandbox/mlops-vertex-kit/serving:latest',\n",
       " '439762652-compute@developer.gserviceaccount.com',\n",
       " 'us-central1-docker.pkg.dev/hyu-ml-sandbox/mlops-vertex-kit/hpo:latest')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_container_image_uri,serving_container_image_uri,custom_job_service_account,hpo_container_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eb569d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"num_leaves_hp_param_min\": 6, \"num_leaves_hp_param_max\": 11, \"max_depth_hp_param_min\": -1, \"max_depth_hp_param_max\": 4}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_additional_args = json.dumps({\n",
    "    'num_leaves_hp_param_min': 6,\n",
    "    'num_leaves_hp_param_max': 11,\n",
    "    'max_depth_hp_param_min': -1,\n",
    "    'max_depth_hp_param_max': 4\n",
    "})\n",
    "train_additional_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d61fc89-15e3-4160-8eb6-8d4aafed5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import Dataset, Input, Metrics, Model, Output\n",
    "\n",
    "@component\n",
    "def worker_pool_specs(project_id: str,\n",
    "    data_region: str,\n",
    "    data_pipeline_root: str,\n",
    "    hpo_container_image_uri: str,\n",
    "    custom_job_service_account: str,\n",
    "    input_dataset: Input[Dataset],\n",
    "    input_data_schema: str) -> list:\n",
    "    \"\"\"\n",
    "    Pass the preprocessed data uri to HPO as a worker pool argument. The vanilla HPO API \n",
    "    doesn't support 'input data' so it's done this way.\n",
    "    \n",
    "    data_preprocess -> dataset.uri -> CMDARGS -> worker_pool_specs -> HPO\n",
    "    \"\"\"\n",
    "    \n",
    "    task_type = 'training'\n",
    "    display_name = 'hpo-pipeline-template'\n",
    "    fields = [field.split(':')[0] for field in input_data_schema.split(';')]\n",
    "    label = fields[-1]\n",
    "    features = ','.join(fields[0:-1])\n",
    "    CMDARGS = [\n",
    "    \"--training_data_uri=\"+str(input_dataset.uri),\n",
    "    \"--training_data_schema=\"+input_data_schema,\n",
    "    \"--label=\"+label,\n",
    "    \"--features=\"+features\n",
    "]\n",
    "\n",
    "    # The spec of the worker pools including machine type and Docker image\n",
    "    worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\"image_uri\": hpo_container_image_uri, \"args\": CMDARGS},\n",
    "    }\n",
    "    ]\n",
    "    \n",
    "    return worker_pool_specs\n",
    "\n",
    "@component(packages_to_install=['google-cloud-firestore==2.3'])\n",
    "def best_hpo_to_args(hpo_best: str,\n",
    "                    project_id: str,\n",
    "                    solution_name: str,\n",
    "                    as_at_date: str) -> str:\n",
    "    \"\"\"\n",
    "    Write the best HPO params to firestore. \n",
    "    We keep the output to chain this component to the hpo_completion step.\n",
    "    \"\"\"\n",
    "    \n",
    "    import json\n",
    "    from google.cloud import firestore\n",
    "    hpo_best = json.loads(hpo_best.replace(\"'\", '\"'))\n",
    "\n",
    "    hpo_best_dict = {}\n",
    "    \n",
    "    for i in hpo_best['parameters']:\n",
    "        hpo_best_dict.update({i['parameterId']: i['value']})\n",
    "    \n",
    "    for i in hpo_best['finalMeasurement']['metrics']:\n",
    "        hpo_best_dict.update({i['metricId']: i['value']})\n",
    "    \n",
    "    db = firestore.Client(project=project_id)\n",
    "    db.collection(solution_name+\"_hpo\").document(\"params\").set(hpo_best_dict,merge=True)\n",
    "    \n",
    "    hpo_best_dict=str(hpo_best_dict).replace(\"'\", '\"')\n",
    "    \n",
    "    return hpo_best_dict\n",
    "\n",
    "@component\n",
    "def hpo_completion(hpo_flags: str) -> str:\n",
    "    \"\"\"\n",
    "    This function doesn nothing but wait to merge all the async HPO jobs so as \n",
    "    to gurantee that the following training module gets the latest params from\n",
    "    firestore for all warehouses.\n",
    "    \"\"\"\n",
    "    return \"true\"\n",
    "\n",
    "def hpo(project_id,\n",
    "         data_region,\n",
    "         data_pipeline_root,\n",
    "         preprocess_task,\n",
    "         display_name,\n",
    "         metric_spec,\n",
    "         parameter_spec,\n",
    "         max_trial_count,\n",
    "         parallel_trial_count,\n",
    "         study_spec_algorithm,\n",
    "         hpo_container_image_uri,\n",
    "         training_data_schema):\n",
    "    \"\"\"\n",
    "    This is not a component function. It's a normal function that generates combines hpo operations. \n",
    "    We return the pipeline operation to chain this component to the hpo_completion step. \n",
    "    \"\"\"\n",
    "    \n",
    "    worker_pool_specs_op = worker_pool_specs(project_id=project_id,\n",
    "    data_region=data_region,\n",
    "    data_pipeline_root=data_pipeline_root,\n",
    "    hpo_container_image_uri=hpo_container_image_uri,\n",
    "    custom_job_service_account=custom_job_service_account,                               \n",
    "    input_dataset=preprocess_task.outputs['output_dataset'],\n",
    "    input_data_schema=training_data_schema\n",
    "    )\n",
    "\n",
    "    tuning_op = HyperparameterTuningJobRunOp(\n",
    "    display_name=display_name,\n",
    "    project=project_id,\n",
    "    location=data_region,\n",
    "    worker_pool_specs=worker_pool_specs_op.output,\n",
    "    study_spec_metrics=metric_spec,\n",
    "    study_spec_parameters=parameter_spec,\n",
    "    max_trial_count=max_trial_count,\n",
    "    parallel_trial_count=parallel_trial_count,\n",
    "    base_output_directory=data_pipeline_root,\n",
    "    study_spec_algorithm=study_spec_algorithm\n",
    "    )\n",
    " \n",
    "    trials_op = hyperparameter_tuning_job.GetTrialsOp(\n",
    "        gcp_resources=tuning_op.outputs[\"gcp_resources\"]\n",
    "    )\n",
    "\n",
    "    best_trial_op = hyperparameter_tuning_job.GetBestTrialOp(\n",
    "        trials=trials_op.output, study_spec_metrics=metric_spec\n",
    "    )\n",
    "    \n",
    "    best_hpo_to_args_op = best_hpo_to_args(best_trial_op.output,\n",
    "                                          project_id=project_id,               \n",
    "                                        as_at_date=datetime.now().strftime('%Y-%m-%d'),\n",
    "                                          solution_name=display_name)\n",
    "    return best_hpo_to_args_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a36e0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name='training-pipeline-template')\n",
    "def pipeline(project_id: str,\n",
    "             data_region: str,\n",
    "             gcs_data_output_folder: str,\n",
    "             input_dataset_uri: str,\n",
    "             training_data_schema: str,\n",
    "             data_pipeline_root: str,\n",
    "             \n",
    "             training_container_image_uri: str,\n",
    "             hpo_container_image_uri: str,\n",
    "             train_additional_args: str,\n",
    "             serving_container_image_uri: str,\n",
    "             custom_job_service_account: str,\n",
    "             hptune_region: str,\n",
    "             hp_config_suggestions_per_request: int,\n",
    "             hp_config_max_trials: int,\n",
    "             \n",
    "             metrics_name: str,\n",
    "             metrics_threshold: float,\n",
    "             \n",
    "             endpoint_machine_type: str,\n",
    "             endpoint_min_replica_count: int,\n",
    "             endpoint_max_replica_count: int,\n",
    "             endpoint_test_instances: str,\n",
    "             \n",
    "             monitoring_user_emails: str,\n",
    "             monitoring_log_sample_rate: float,\n",
    "             monitor_interval: int,\n",
    "             monitoring_default_threshold: float,\n",
    "             monitoring_custom_skew_thresholds: str,\n",
    "             monitoring_custom_drift_thresholds: str,\n",
    "             \n",
    "             machine_type: str = \"n1-standard-4\",\n",
    "             accelerator_count: int = 0,\n",
    "             accelerator_type: str = 'ACCELERATOR_TYPE_UNSPECIFIED',\n",
    "             vpc_network: str = \"\",\n",
    "             enable_model_monitoring: str = 'False'):\n",
    "    \n",
    "    display_name = 'hpo-pipeline-template'\n",
    "    metric_spec = hyperparameter_tuning_job.serialize_metrics({\"au_roc\": \"maximize\"})\n",
    "    parameter_spec = hyperparameter_tuning_job.serialize_parameters(\n",
    "    {\n",
    "        \"num_boost_round\": aiplatform.hyperparameter_tuning.DiscreteParameterSpec(\n",
    "            values=[100, 200], scale=None\n",
    "        ),\n",
    "        \"min_data_in_leaf\": aiplatform.hyperparameter_tuning.DiscreteParameterSpec(\n",
    "            values=[5, 10], scale=None\n",
    "        ),\n",
    "    })\n",
    "    max_trial_count=4\n",
    "    parallel_trial_count=4\n",
    "    study_spec_algorithm='GRID_SEARCH'\n",
    "    \n",
    "\n",
    "    dataset_importer = kfp.v2.dsl.importer(\n",
    "      artifact_uri=input_dataset_uri,\n",
    "      artifact_class=Dataset,\n",
    "      reimport=False)\n",
    "\n",
    "    preprocess_task = preprocess_op(\n",
    "      project_id=project_id,\n",
    "      data_region=data_region,\n",
    "      gcs_output_folder=gcs_data_output_folder,\n",
    "      gcs_output_format=\"CSV\",\n",
    "      input_dataset=dataset_importer.output)\n",
    "    \n",
    "    hpo_op = hpo(project_id,\n",
    "             data_region,\n",
    "             data_pipeline_root,\n",
    "             preprocess_task,\n",
    "             display_name,\n",
    "             metric_spec,\n",
    "             parameter_spec,\n",
    "             max_trial_count,\n",
    "             parallel_trial_count,\n",
    "             study_spec_algorithm,\n",
    "             hpo_container_image_uri,\n",
    "             training_data_schema)\n",
    "    \n",
    "    hpo_completion_op = hpo_completion(str(hpo_op.output))\n",
    "    \n",
    "    with dsl.Condition(\n",
    "         hpo_completion_op.output==\"true\",\n",
    "        name=\"train_model\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        We use the condition module to check if all HPO jobs for different warehouse are finished so as to\n",
    "        kick off the training step at the right time.\n",
    "        \"\"\"\n",
    "\n",
    "        train_task = train_op(\n",
    "          project_id=project_id,\n",
    "          data_region=data_region,\n",
    "          data_pipeline_root=data_pipeline_root,\n",
    "          input_data_schema=training_data_schema,\n",
    "          training_container_image_uri=training_container_image_uri,\n",
    "          train_additional_args=train_additional_args,\n",
    "          serving_container_image_uri=serving_container_image_uri,\n",
    "          custom_job_service_account=custom_job_service_account,\n",
    "          input_dataset=preprocess_task.outputs['output_dataset'],\n",
    "          machine_type=machine_type,\n",
    "          accelerator_count=accelerator_count,\n",
    "          accelerator_type=accelerator_type,\n",
    "          hptune_region=hptune_region,\n",
    "          hp_config_max_trials=hp_config_max_trials,\n",
    "          hp_config_suggestions_per_request=hp_config_suggestions_per_request,\n",
    "          vpc_network=vpc_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5fba56",
   "metadata": {},
   "source": [
    "### Compile and run the end-to-end ML pipeline\n",
    "With our full pipeline defined, it's time to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "794c18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, \n",
    "    package_path=\"hpo_pipeline_job.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575b575f",
   "metadata": {},
   "source": [
    "Next, instantiate an API client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ab349ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=project_id,\n",
    "    region=pipeline_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951f42d8",
   "metadata": {},
   "source": [
    "Next, kick off a pipeline run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0423941b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"VWT\": 3.6216, \"SWT\": 8.6661, \"KWT\": -2.8073, \"Entropy\": -0.44699, \"Class\": \"0\"}, {\"VWT\": 4.5459, \"SWT\": 8.1674, \"KWT\": -2.4586, \"Entropy\": -1.4621, \"Class\": \"0\"}, {\"VWT\": 3.866, \"SWT\": -2.6383, \"KWT\": 1.9242, \"Entropy\": 0.10645, \"Class\": \"0\"}, {\"VWT\": -3.7503, \"SWT\": -13.4586, \"KWT\": 17.5932, \"Entropy\": -2.7771, \"Class\": \"1\"}, {\"VWT\": -3.5637, \"SWT\": -8.3827, \"KWT\": 12.393, \"Entropy\": -1.2823, \"Class\": \"1\"}, {\"VWT\": -2.5419, \"SWT\": -0.65804, \"KWT\": 2.6842, \"Entropy\": 1.1952, \"Class\": \"1\"}]'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_instances = json.dumps([\n",
    "\t\t{\"VWT\":3.6216,\"SWT\":8.6661,\"KWT\":-2.8073,\"Entropy\":-0.44699,\"Class\":\"0\"},\n",
    "\t\t{\"VWT\":4.5459,\"SWT\":8.1674,\"KWT\":-2.4586,\"Entropy\":-1.4621,\"Class\":\"0\"},\n",
    "\t\t{\"VWT\":3.866,\"SWT\":-2.6383,\"KWT\":1.9242,\"Entropy\":0.10645,\"Class\":\"0\"},\n",
    "\t\t{\"VWT\":-3.7503,\"SWT\":-13.4586,\"KWT\":17.5932,\"Entropy\":-2.7771,\"Class\":\"1\"},\n",
    "\t\t{\"VWT\":-3.5637,\"SWT\":-8.3827,\"KWT\":12.393,\"Entropy\":-1.2823,\"Class\":\"1\"},\n",
    "\t\t{\"VWT\":-2.5419,\"SWT\":-0.65804,\"KWT\":2.6842,\"Entropy\":1.1952,\"Class\":\"1\"}\n",
    "\t\t])\n",
    "test_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a3fe7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/training-pipeline-template-20230211084202?project=hyu-ml-sandbox\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_params = {\n",
    "    'project_id': project_id,\n",
    "    'data_region': data_region,\n",
    "    'gcs_data_output_folder': gcs_data_output_folder,\n",
    "    'input_dataset_uri': input_dataset_uri,\n",
    "    'training_data_schema': training_data_schema,\n",
    "    'data_pipeline_root': data_pipeline_root,\n",
    "    \n",
    "    'training_container_image_uri': training_container_image_uri,\n",
    "    'hpo_container_image_uri': hpo_container_image_uri,\n",
    "    'train_additional_args': train_additional_args,\n",
    "    'serving_container_image_uri': serving_container_image_uri,\n",
    "    'custom_job_service_account': custom_job_service_account,\n",
    "    'hptune_region':\"us-central1\",\n",
    "    'hp_config_suggestions_per_request': 5,\n",
    "    'hp_config_max_trials': 30,\n",
    "    \n",
    "    'metrics_name': 'au_prc',\n",
    "    'metrics_threshold': 0.4,\n",
    "    \n",
    "    'endpoint_machine_type': 'n1-standard-4',\n",
    "    'endpoint_min_replica_count': 1,\n",
    "    'endpoint_max_replica_count': 1,\n",
    "    'endpoint_test_instances': test_instances,\n",
    "    \n",
    "    'monitoring_user_emails': 'simon19891101@google.com',\n",
    "    'monitoring_log_sample_rate': 0.8,\n",
    "    'monitor_interval': 3600,\n",
    "    'monitoring_default_threshold': 0.3,\n",
    "    'monitoring_custom_skew_thresholds': 'VWT:.5,SWT:.2,KWT:.7,Entropy:.4',\n",
    "    'monitoring_custom_drift_thresholds': 'VWT:.5,SWT:.2,KWT:.7,Entropy:.4',\n",
    "    'enable_model_monitoring': 'True'\n",
    "}\n",
    "\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=\"hpo_pipeline_job.json\", \n",
    "    pipeline_root=pipeline_root,\n",
    "    parameter_values=pipeline_params,\n",
    "    service_account=custom_job_service_account,\n",
    "    enable_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc5586d2-f1a5-466c-aec7-4398e29c0b43",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hptune'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_6067/2418256561.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhptune\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pinfo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hpt.report_hyperparameter_tuning_metric'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hptune'"
     ]
    }
   ],
   "source": [
    "import hptune as hpt\n",
    "hpt.report_hyperparameter_tuning_metric?"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
